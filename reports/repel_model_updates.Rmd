---
title: Repel Model Updates
author: Ernest Guevarra, Emma Mendelsohn and Noam Ross, EcoHealth Alliance
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  warning = FALSE, 
  fig.height = 8, 
  fig.width = 12
)

## Load packages and project specific functions ----
library(ggplot2)
library(yardstick)
source(here::here("packages.R"))
for (f in list.files(here::here("R"), full.names = TRUE)) source (f)
```

This report documents the evolution of the REPEL livestock model as we make updates and improvements upon the replicated original model. It will evolve as we continue to make changes to the data and model. The "replicated original model" Phase II model report is available via Release 0.2.3 https://github.com/ecohealthalliance/repel2-battelle/releases/tag/v0.2.3

The following changes are reflected in this report:

1. WAHIS disease and taxa names have been fully cleaned and standardized
2. No longer remove diseases that only occur in a single country from the model dataset
3. We filter out diseases that affect non-livestock for which we do not have population data: bees, dogs, wolves, chimps
4. We no longer exclude birds from wildlife migration metrics
5. We added a monthly time period offset for outbreak predictions
6. Use outbreak table to fill in NAs in wahis events closing dates
7. We shifted the model to a yearly time interval
8. Removed non-livestock related commodities from the Comtrade dataset
9. Filter training data to 2006-2022. Also fixed a bug from the offset refactoring that might have been removing some COMTRADE/FAO data.
10. Updating `targets` package to version 1.4.0 changed pseudo-random number generator seeds, which resulted in a different train/validation split. 
11. Add two predictor variables `disease_present_anywhere` for whether the disease is present anywhere in the world and `outbreak_previous` for whether the disease has ever occurred previously in the given country. 
12. Downgraded `targets` package to version 1.3.2 due to conflicts with our internal AWS-based collaborative workflow. This again changes pseudo-random number generator seeds.
13. Log transform comtrade, fao, wildlife migration
14. Remove NOWCAST model imputation for endemic events


```{r}
## Load required targets for current/updated augmented dataset ----
tar_load(
  c(repel_model,
    repel_confusion_matrix, 
    repel_performance,
    repel_calibration,
    repel_calibration_plot,
    repel_calibration_table,
    repel_calibration_n_within_range,
    repel_validation_predict
  )
)
```


## Model Summary


```{r}
summary(repel_model)
```

## Model Performance

### Confusion Matrix

Model predictions are in the form of probabilities from 0-1.
We assumed that a prediction of \>= 0.5 indicates that the model predicted an outbreak event.

While confusion matrices and their summary statistics are standard metrics for binary models, they are limited for evaluating this model, which predicts rare events that generally have a probability well below 0.5.
Metrics which weight negative events reflect the large number of zeroes in the dataset.
Metrics focusing only on rare outbreak events (Kappa, Negative Predictive Value, Matthews correlation coefficient) reflect performance in very small number of cases where monthly import risk is above 50%.
Calibration curves (next section) provide a better measure of rare events.

```{r}
repel_confusion_matrix |> 
  ggplot2::autoplot(type = "heatmap") +
  ggplot2::labs(y = "Predicted Outbreak Event", x = "Observed Outbreak Event") +
  ggplot2::theme(text = element_text(size = 15))
```

```{r}
repel_performance |> 
  knitr::kable(
  ) |>
  kableExtra::kable_paper(full_width = FALSE, position = "left")

# Kappa is a similar measure to accuracy(), but is normalized by the accuracy that would be expected by chance alone and is very useful when one or more classes have large frequency distributions.

```

### Calibration Curve

We assessed model predictions as probabilities against observed outbreak rates in the validation set.
We grouped predictions into 30 quantile-based bins.
We compared the average prediction of each bin to observed outbreak rates within the bin (represented as binomial probabilities and 95% confidence intervals).
Each prediction represents the expectation of an outbreak of a given disease in a country in a given month.
This assessment evaluates the reliability of predictions for rare events: given a predicted probability of a rare outbreak, how well is that probability borne out as a fraction of times that outbreaks actually occurred in the validation data?


```{r}
# interval breaks it into even groups by predictor value, eg 0.1-0.2
# width is just another way to specify intervals
# number breaks even number of values per group (ie quantile based)


options(scipen = 999)
repel_calibration_plot

```

```{r}
repel_calibration_table |> 
  knitr::kable(caption = "values are per 10,000 potential events") |> 
  kableExtra::kable_paper(full_width = FALSE, position = "left") 

n_within <- repel_calibration_n_within_range

```

Across the range of predictions, the average predicted probability matches the observed fraction of events (by falling within binomial confidence intervals) in `r n_within` of `r nrow(repel_calibration_table)` bins.

### Reliability Diagram
From https://www.pnas.org/doi/full/10.1073/pnas.2016191118

Uses the pool-adjacent-violators algorithm to generate optimally binned, reproducible, and provably statistically consistent reliability diagrams.

Mean Score (S) is the event rate in the dataset (` mean(as.integer(repel_validation_predict_crop$outbreak_start))`)

Uncertainty (UNC) is the mean score of a constant prediction at the value of the average observation. It is the highest possible mean score of a calibrated prediction method. It measures the inherent difficulty of the prediction problem, but does not depend on the forecast under consideration.

Discrimination (DSC) is UNC minus the mean score of the PAV-recalibrated forecast values. A small value indicates a low information content (low signal) in the original forecast values. Increasing value indicates model improvement. 

Miscalibration (MCB) is S minus the mean score of the PAV-recalibrated forecast values. A high value indicates that predictive performance of the prediction method can be improved by recalibration. Decreasing value indicates model improvement. 

These measures are related by the following equation:
S=MCBâˆ’DSC+UNC.


```{r}
rd <- reliabilitydiag::reliabilitydiag(x = repel_validation_predict$predicted,
                                       y = as.integer(repel_validation_predict$outbreak_start))

rd_plot <- reliabilitydiag::autoplot(rd)

rd_plot +
  ggplot2::scale_x_sqrt() +
  ggplot2::scale_y_sqrt()

summary(rd) |>
  dplyr::select(-forecast) |>
  knitr::kable() |>
  kableExtra::kable_paper(full_width = FALSE, position = "left")
```


<details>

<summary>Session info</summary>

-   Built at: `r Sys.time()`
-   Last git commit hash: `r gert::git_commit_id()`

</details>

