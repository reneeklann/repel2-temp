---
title: "REPEL Livestock and Crop Model Pipeline"
author: "EcoHealth Alliance"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    keep_md: true
vignette: >
  %\VignetteIndexEntry{Model Pipeline Instructions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette is an overview of the full pipeline for the livestock and crop REPEL models, from data ingest and processing to model fitting and predictions. The project final technical report can be found in `reports/final-report.`

# Project setup

## Downloading the repository

The project GitHub repository is available at [https://github.com/ecohealthalliance/repel2-battelle](https://github.com/ecohealthalliance/repel2-battelle).

In the terminal, the repository can be cloned using HTTPS:

```         
git clone https://github.com/ecohealthalliance/repel2-battelle
```

or using SSH:

```         
git clone git@github.com:ecohealthalliance/repel2-battelle.git
```

You also have the option of downloading a zipped version of the repository from GitHub. Unzip the file into a preferred directory on your machine.

## Setting environment variables

Environment variables are used to store authentication keys for data sources and to specify model settings. 

Variables can be set manually before running the data ingest pipeline. For example, in R: 

```
Sys.setenv(VARIABLE_NAME = YOUR_VARIABLE_VALUE)
```
However, we suggest storing environment variables in an `.env` file in the root directory of the repository. This way, environment variables will always be loaded prior to running the pipeline. 

### Authentication keys

Some of the data sources require authentication keys to access the data programmatically. The following keys should be saved as environment variables. 

```         
IUCN_REDLIST_KEY=YOUR_IUCN_REDLIST_KEY_HERE

COMTRADE_PRIMARY=YOUR_COMTRADE_PRIMARY_KEY_HERE
COMTRADE_SECONDARY=YOUR_COMTRADE_SECONDARY_KEY_HERE

DOLT_TOKEN=YOUR_DOLTHUB_TOKEN_HERE

EPPO_TOKEN=YOUR_EPPO_TOKEN_HERE
```
Below we provide instructions for obtaining these keys.

#### The International Union for Conservation of Nature (IUCN) Red List of Threatened Species API token

The IUCN provides an API to programmatically access the IUCN Red List of Threatened Species.
The required API token can be generated by making a request at <https://apiv3.iucnredlist.org/api/v3/token> detailing reason for API use.
Request is sent to IUCN and approval along with the API token is provided to the email address used during registration.
This may take several days.

*Due to delays in obtaining IUCN tokens, the IUCN data file is now saved as part of this data repository. It is no longer necessary to obtain a token to be able to run the pipeline.*

#### United Nations Comtrade Database API token/key

The [UN Comtrade Database](https://comtradeplus.un.org/) can be accessed programmatically using its API.
The API token/key can be obtained as follows:

1.  Create a UN Comtrade Database account at this [link](https://unb2c.b2clogin.com/unb2c.onmicrosoft.com/b2c_1a_signup_signin_comtrade/oauth2/v2.0/authorize?client_id=85644091-2534-4703-a6e9-456533d03b2d&scope=openid%20https%3A%2F%2Funb2c.onmicrosoft.com%2Fcomtradeapibe%2Fcomtradeapifunction.write%20profile%20offline_access&redirect_uri=https%3A%2F%2Fcomtradeplus.un.org&client-request-id=3927c60f-62b4-49eb-838e-4a0eff479045&response_mode=fragment&response_type=code&x-client-SKU=msal.js.browser&x-client-VER=2.22.0&x-client-OS=&x-client-CPU=&client_info=1&code_challenge=VHldePVQmSWkhv_iK9_kBjUGPBLlGZA3S9vqiW74wwQ&code_challenge_method=S256&nonce=3905f277-db63-4746-93e7-264cb061b48a&state=eyJpZCI6IjAwNGMzYjU0LWRjMzMtNGNmOC1hOTg4LWUzYzJjNjc3NmU0OCIsIm1ldGEiOnsiaW50ZXJhY3Rpb25UeXBlIjoicmVkaXJlY3QifX0%3D)

2.  Sign in to the [UN Comtrade Database API portal](https://comtradedeveloper.un.org/signin?returnUrl=%2F) and then proceed to the **Products** tab (<https://comtradedeveloper.un.org/products>)

3.  From the **Products** page, select the **Free APIs** option (<https://comtradedeveloper.un.org/product#product=free>)

4.  In the text entry dialog box, enter `comtrade-v1` and then click the `Subscribe` button

After these steps, the page should now show you two subscription keys called primary and secondary which are the API tokens/keys needed for API access.

#### World Animal Health Information System (WAHIS) data via EcoHealth Alliance's DoltHub repository

EcoHealth Alliance routinely curates animal health information on animal disease events and outbreaks from WOAH's [WAHIS website](https://wahis.woah.org/#/home) using [DoltHub](https://www.dolthub.com).
The curated database is found [here](https://www.dolthub.com/repositories/ecohealthalliance/wahisdb) and is openly accessible.
Programmatically, access can be gained using a DoltHub API key.
A DoltHub API key can be created as follows:

1.  Create a DoltHub account

2.  Log in to your DoltHub account

3.  Create an API token in your [settings](https://www.dolthub.com/settings/tokens) on DoltHub

#### European and Mediterranean Plant Protection Organization (EPPO) Data Services

EPPO Data Services provides an API to programmatically access the database used for plant disease name standardization.
An API token can be generated by registering for an account at <https://data.eppo.int/user/register>.
Immediately after registration, the token can be copied from the [user dashboard](https://data.eppo.int/user/), under the **Tokens API** header.

### Project settings

In addition to authentication keys, environment variables can be used to store project settings. For example, the user can set the following variables to use AWS for object storage:
```
AWS_REGION=YOUR_AWS_REGION_HERE
AWS_BUCKET_ID=YOUR_AWS_BUCKET_ID_HERE
AWS_ACCESS_KEY_ID=YOUR_AWS_ACCESS_KEY_ID_HERE
AWS_SECRET_ACCESS_KEY=YOUR_AWS_SECRET_ACCESS_KEY_HERE
```

Additional settings related to model fitting and prediction are described in their respective sections below. 

## R dependencies

This pipeline was created using `r R.version$version.string`.
This project uses the `{renv}` framework to record R package dependencies and versions.
Packages and versions used are recorded in `renv.lock` and code used to manage dependencies is in `renv/` and other files in the root project directory.
On starting an R session in the working directory, install R package dependencies:

```         
renv::restore()
```

or using the following command in Terminal

```         
Rscript -e 'renv::restore()'
```

## Targets workflow

This pipeline uses the [`targets` package](https://books.ropensci.org/targets/) as a build system to resolve dependencies and cache results. This is similar conceptually to `make`, with the `_targets.R` file being the equivalent of a `Makefile`.
With `targets`, the user can build individual parts of the pipeline and they will be cached when running in the future.

<noscript>

```{=html}
<style>
 .withscript {display:none;}
</style>
```
</noscript>

[This diagram shows all the components of the workflow for the livestock model:]{.withscript}

```{r livestock_mermaid, eval = TRUE, echo = FALSE, message = FALSE, warning = FALSE, results = "asis"}
# mer <- targets::tar_mermaid(targets_only = TRUE,
#                             names = repel_full_pipeline,
#                             outdated = FALSE, 
#                             legend = FALSE, color = FALSE, 
#                             exclude = c("readme", ends_with("_targets")))
# cat(
#   '```{=html}',
#   '<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>',
#   '<script>mermaid.initialize({startOnLoad:true});</script>',
#   '<style>.classLabel.label {}</style>',
#   '<div class="mermaid withscript" style="width:130%;transform: translateX(-15%);">',
#   mer[1], 
#   #'Objects([""Objects""]) --- Functions>""Functions""]',
#   'subgraph identifier[" "]',
#   mer[4:length(mer)],
#   'classDef default font-size:25px;',
#   '</div>',
#   '```',
#   sep = "\n"
# )
```

<noscript>

```{=html}
<style>
 .withscript {display:none;}
</style>
```
</noscript>

[This diagram shows all the components of the workflow for the crop model:]{.withscript}

```{r crop_mermaid, eval = TRUE, echo = FALSE, message = FALSE, warning = FALSE, results = "asis"}
# mer <- targets::tar_mermaid(targets_only = TRUE,
#                             names = repel_full_crop_pipeline,
#                             outdated = FALSE, 
#                             legend = FALSE, color = FALSE, 
#                             exclude = c("readme", ends_with("_targets")))
# cat(
#   '```{=html}',
#   '<script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>',
#   '<script>mermaid.initialize({startOnLoad:true});</script>',
#   '<style>.classLabel.label {}</style>',
#   '<div class="mermaid withscript" style="width:130%;transform: translateX(-15%);">',
#   mer[1], 
#   #'Objects([""Objects""]) --- Functions>""Functions""]',
#   'subgraph identifier[" "]',
#   mer[4:length(mer)],
#   'classDef default font-size:25px;',
#   '</div>',
#   '```',
#   sep = "\n"
# )
```

## Docker

We have assembled a Docker image with necessary dependencies to run the pipeline, and a convenience shell script (`tar_make.sh`) for building pipeline steps in the console.
To build the image locally and use it to run the pipeline, run the following

```         
docker build . -t ecohealthalliance/repel2
docker run -v "$(pwd):/repel2" ecohealthalliance/repel2 ./tar_make.sh '<TARGET_NAME>'
```

`<TARGET_NAME>` can be any target and can also be `everything()` or other selection commands such as `tidyr::contains(c("woah", "wahis", "iucn"))`.

In addition, we have provided a tarball of the Docker image.
Instead of building it, you can load it with

```         
docker load < repelcontainer_vXXX.tar.gz
```

# Data ingest and processing

The data ingest pipeline retrieves the datasets used by the REPEL model from their various sources.
These datasets and their respective sources are:

```{r, echo = FALSE}
dataset <- tibble::tribble(~"Name", ~"Frequency", ~"Dimension", ~"Source", ~"Model(s)",

"Country shared borders", "static", "bilateral", "[CIA World Factbook archive](https://www.cia.gov/about/archives/download/factbook-2020.zip)", "both",
"Human population", "yearly", "per country", "[World Bank](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)", "both",
"Gross Domestic Product (GDP)", "yearly", "per country","[World Bank](https://data.worldbank.org/indicator/NY.GDP.MKTP.CD)", "both",
"Wildlife migration", "static", "bilateral", "[International Union for Conservation of Nature](https://www.iucnredlist.org/)", "both",

"Animal disease events and outbreaks", "yearly", "per country", "[World Organisation for Animal Health](https://wahis.woah.org/#/home)", "livestock",
"Veterinarian population", "yearly", "per country", "[World Organisation for Animal Health](https://wahis.woah.org/#/home)", "livestock",

"Plant pest reports", "yearly", "per country", "[European and Mediterranean Plant Protection Organization](https://gd.eppo.int/reporting/)", "crop",
"Plant pest reports", "yearly", "per country", "[International Plant Protection Convention](https://www.ippc.int/en/countries/all/pestreport/)", "crop",
"Plant pest reports", "yearly", "per country", "[North American Plant Protection Organization](https://www.pestalerts.org/nappo/official-pest-reports/)", "crop",

"Taxa population", "yearly", "per country", "[Food and Agriculture Organization](https://www.fao.org/faostat/en/#data/QCL)", "livestock",
"Livestock trade", "yearly", "bilateral", "[Food and Agriculture Organization](https://www.fao.org/faostat/en/#data/QCL)", "livestock",
"Crop production", "yearly", "per country", "[Food and Agriculture Organization](https://www.fao.org/faostat/en/#data/QCL)", "crop",
"Crop trade", "yearly", "bilateral", "[Food and Agriculture Organization](https://www.fao.org/faostat/en/#data/TCL)", "crop",

"Agricultural product trade", "yearly", "bilateral", "[United Nations Comtrade Database](https://comtradeplus.un.org/)", "both"
)

dataset |>
  knitr::kable(
    row.names = FALSE,
    caption = "REPEL datasets with respective data sources",
    escape = FALSE
  ) |>
  kableExtra::kable_paper() 
```

The entire data ingest and processing pipeline can be run using the following command in the R console. 

```         
targets::tar_make(c(augmented_livestock_data_aggregated, augmented_crop_data_aggregated))
```

or using the following command in Terminal

```         
Rscript -e 'targets::tar_make(c(augmented_livestock_data_aggregated, augmented_crop_data_aggregated))'
```

The `augmented_livestock_data_aggregated` or `augmented_crop_data_aggregated` target endpoint is the full model dataset, which combines all predictor and outcome data sources, used for training and validation.
Note that these may take over a day to run due to the data download steps (particularly Comtrade).
Below we provide guidance for testing subsets of the data ingest pipeline with faster run times.
Note, running `tar_make()` without specifying a target will run all livestock and crop targets.

## Pipeline testing

Given specific requirements (such as API tokens) and length of run times described above, we recommend the following steps for someone trying out this data ingest and processing pipeline for the first time or for someone reproducing the outputs:

1. Test that the pipeline works as described by running a data source pipeline that doesn't require any tokens

For the livestock pipeline, we recommend the pipelines for data retrieved from the FAO, UN Statistics Division, CIA World Factbook, and the World Bank. We would expect this pipeline to complete in \~20 minutes.
To run these, the following command can be used in the R console:

```         
targets::tar_make(c(country_yearly_human_population, country_yearly_gdp, connect_static_shared_borders, country_yearly_taxa_population, connect_yearly_fao_trade_livestock, country_yearly_vet_population))
```

For the crop pipeline, we recommend the pipelines for data retrieved from the FAO, UN Statistics Division, CIA World Factbook, the World Bank, EPPO, IPPC, and NAPPO. We would expect this pipeline to complete in \~4 hours. To run these, the following command can be used in the R console:

```         
targets::tar_make(c(country_yearly_human_population, country_yearly_gdp, connect_static_shared_borders, country_yearly_crop_production, connect_yearly_fao_trade_crop, eppo_index_processed, ippc_table_processed, nappo_table_processed))
```

Note that `source("tar_plans/_targets_data_downloads_crop.R")` and `source("tar_plans/_targets_data_processing_crop.R")` are currently commented out in the `_targets.R` file. This needs to be changed in order to run the crop data ingest and processing pipeline.

This step will give an indication that the general pipeline works as expected if the run completes without errors.
For a faster test, you could run the world bank GDP data pipeline, which should complete in seconds: `targets::tar_make(country_yearly_gdp)`.

2. Test the pipeline for the steps requiring authentication keys/tokens

For the livestock pipeline, the steps requiring authentication keys/tokens that complete fast are for data retrieved from the IUCN and WOAH (via EcoHealth Alliance's DoltHub database). We would expect this pipeline to complete in \~10 minutes.
To run these, the following command can be used in the R console:

```         
targets::tar_make(c(connect_static_wildlife_migration, connect_livestock_outbreaks))
```

For the crop pipeline, the step requiring authentication keys/tokens that completes fast is for disease name standardization using the EPPO Global Database. We would expect this pieline to complete in ~30 minutes. To run this, the following command can be used in the R console:

```         
targets::tar_make(pest_names_standardized)
```

This step will test whether you have set up your authentication keys appropriately.

3. Run the pipeline for the UN Comtrade database

The remaining source, the UN Comtrade database, requires authentication and also runs the longest (Based on API call limits and depending on server traffic, it could be 2-4 days).
We recommend running this last, using this command in the R console:

```         
targets::tar_make(connect_yearly_comtrade_livestock)
targets::tar_make(connect_yearly_comtrade_crop)
```

**Addressing server errors with the UN Comtrade download pipeline**

The download pipeline for the trade data through the UN Comtrade data API uses authentication keys for a basic individual free subscription.
This type of subscription has specified [rate limits](https://unstats.un.org/wiki/display/comtrade/New+Comtrade+FAQ+for+First+Time+Users#NewComtradeFAQforFirstTimeUsers-Andwhat'sthedownloadcapacityforsubscriptionusers?) which has been taken into account in the pipeline.
However, performing bulk download with a basic individual free subscription is known to infrequently produce `HTTP 500 Internal Server Error`.
To avoid this, UN Comtrade recommends a premium individual or premium institution subscription which allows access to UN Comtrade's bulk API (click [here](https://unstats.un.org/wiki/display/comtrade/New+Comtrade+FAQ+for+First+Time+Users#NewComtradeFAQforFirstTimeUsers-Andwhat'sthedownloadcapacityforsubscriptionusers?) for details on the subscription packages).

The user can prevent these infrequent errors from stopping the full pipeline by setting an environment variable 
```
TARGETS_ERROR="null"
```
Because the Comtrade download pipeline uses dynamic targets branching, this setting allows the pipeline to skip over any failed branches and continue to run subsequent steps using only the data that has been successfully downloaded.
Then, a subsequent call to re-run the pipeline will run the downloads _only_ for the remaining data that have not been downloaded yet.
We recommend re-running the download pipeline at a later time after an `HTTP 500 Internal Server Error`, particularly at less busy times (e.g. evenings, weekends).

## Detecting changes in data sources
The workflow has been set up to detect that the data from source has changed and these steps will re-run. For the Comtrade data and the FAO production and trade data specifically, we have noted that during the period of developing this pipeline these data sources changed field names, which caused errors. We have now put in place a check system that will detect these changes and provide a more informative error message regarding this during the data processing step and recommend that data processing functions be updated/refactored to adjust for the new field names. For Comtrade, we have put in place a check in the pipeline that runs prior to the start of the Comtrade download step and provides a warning that Comtrade field names have changed. Because the Comtrade download step takes the most time in the pipeline, the data check doesnâ€™t stop the pipeline from performing the download but instead gives the warning in advance to allow for updating/refactoring of functions while the download is progressing. For all other data, the same checks are performed in case any change happens, but for the most part we expect these to remain the same.


## Data storage

All raw data from the various data sources are downloaded and then stored before any processing or standardization inside the `data-raw/` directory.

Unless the user has enabled AWS object storage (see Project settings above), all processed data are stored as [qs](https://cran.r-project.org/web/packages/qs/vignettes/vignette.html) files in the `_targets/objects/` directory.

Data objects can be viewed using the `targets` package. For example, to view GDP data:

```
targets::tar_read(country_yearly_gdp)
```

Data schemas and descriptions are available as csv files in the `inst/` directory.

-   `inst/data_dictionary_raw.csv` contains schemas for the raw downloaded data files, and

-   `inst/data_dictionary.csv` contains schemas for the processed data

In the crop pipeline, several intermediate data products are stored in the repository to allow the user to skip the step of regenerating resource-intensive steps. These files are automatically incorporated into the pipeline, and the code to generate these files is currently commented out in `_targets.R`.

-   `data-raw/comtrade-crop/connect_yearly_comtrade_crop.csv`

-   `data-raw/crop-data-extracted/extracted_data_complete.csv`

-   `data-raw/crop-disease-lookup/crop_report_index.csv`

 `repel1-extracts` contains model objects and predictions from the Phase I REPEL project.



## Crop disease data extraction

The crop pipeline uses OpenAI GPT-4 large-language model (LLM) (specifically, gpt-4-1106-preview ), accessed via Web API, to extract data from free text crop disease outbreak reports. The extraction was run with EPPO reports through December 2023, and IPPC and NAPPO reports through February 2024. The output is saved as parquet files in the `data-raw/crop-data-extracted/` directory. The complete output, including manually reviewed data, is saved as `extracted_data_complete.csv` in the same directory.

Future updates can be performed with NLP or manually.

### NLP approach

To run the extractions with NLP, you need an OpenAI account with an API key, saved as 
```
OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE
```

Currently, `source("tar_plans/_targets_data_extraction_crop.R")` is commented out in the `_targets.R` file, to bypass the extraction pipeline and use the csv file of extracted data in subsequent targets. This needs to be uncommented for the extraction pipeline to rerun.

Make a backup of `extracted_data_complete.csv`, then run
```
targets::tar_make(extracted_data_complete)
```

### Manual approach

To perform manual updates, run
```
targets::tar_load(extracted_data)
extracted_data <- read.csv(extracted_data)
extracted_data <- extracted_data |> dplyr::mutate(date_published = lubridate::as_date(date_published))

targets::tar_make(crop_report_index)
targets::tar_load(crop_report_index)
crop_report_index <- crop_report_index |> dplyr::mutate(issue = as.integer(issue))

new_reports <- dplyr::anti_join(crop_report_index, extracted_data, by = dplyr::join_by(source, pest, preferred_name, country, date_published, year_published, issue, eppo_unique_id, url))

write.csv(new_reports, "data-raw/crop-data-extracted/data_update_manual_extraction.csv")
```
This will create a csv of new records since the pipeline was last run. Add the columns `disease_manual`, `year_manual`, `month_manual`, `presence_manual`, and `manually_extracted`.

Then read each report and record:

- The scientific name of the disease or pest in the column `disease_manual`. Refer to the [EPPO Global Database](https://gd.eppo.int/) for the preferred name.

- The year of the outbreak in the column `year_manual`. The year should be in YYYY format, or NA if not mentioned in the report.

- The month of the outbreak in the column `month_manual`. Use the full English name of the month, or NA if not mentioned in the report.

- The disease status in the country in the column `presence_manual`. The status can be "present", "absent", or NA if it is unclear from the report.

Note that some EPPO reports include multiple diseases and/or countries, so double-check the `preferred_name` and `country` columns to make sure the data is recorded in the right place.
After reading each report, enter TRUE in the column `manually_extracted`.

Then make a backup of `data-raw/crop-data-extracted/extracted_data_complete.csv` and run the following
```
extracted_data_complete_old <- read.csv("data-raw/crop-data-extracted/extracted_data_complete.csv")
data_update_manual_extraction <- read.csv("data-raw/crop-data-extracted/data_update_manual_extraction.csv")
extracted_data_complete_new <- dplyr::bind_rows(extracted_data_complete_old, data_update_manual_extraction)
write.csv(extracted_data_complete_new, "data-raw/crop-data-extracted/extracted_data_complete.csv")
```

# Model fitting

Data are aggregated to the yearly time scale, such that we are predicting outbreak probabilities using 12-month windows. For example, we predict disease outbreak probability for Jan-Dec 2022 based on conditions in Jan-Dec 2021. 

The full dataset is randomly split into training (80%) and validation (20%) data. The training data are scaled and used to fit a linear mixed effects model, with random effects for each disease. 

The full pipeline including data ingest, fitting, and prediction can be run with the following code. The model fitting step takes ~1 hr for the livestock model and ~24 hrs for the crop model. 

```
targets::tar_make(repel_full_pipeline)
targets::tar_make(repel_full_crop_pipeline) 
```

All elements of the model pipeline can be inspected. This may be especially useful to view the model dataset:

```  
targets::tar_read(augmented_livestock_data_aggregated)
targets::tar_read(augmented_crop_data_aggregated)
```  
Or the model object:

```  
targets::tar_read(repel_model)
targets::tar_read(repel_model_crop)
```

## Model reports

-   `reports/comparison_repel_models.html` was developed to confirm that our Phase II pipeline can reproduce the dataset and model from the previous Phase I pipeline. Note that to match the Phase I approach, this report represents the Phase II model on the monthly time scale.

-   `reports/repel_model_updates.html` presents livestock model results with a high-level list of the changes that have been made to the model since reproducing the Phase I pipeline.

-   `reports/repel_crop_model_updates.html` presents crop model results.

## Model cache and retraining

The model was trained and validated on data ending in September 2022. This version and associated input data and validation steps are preserved in the [Data Cache](https://github.com/ecohealthalliance/repel2-battelle/releases/tag/data-cache) GitHub version release. To avoid unnecessarily refitting the model when data is updated for predictions, the pipeline can be run with cached data. This allows new data downloads, processing, and predictions to run with a fixed version of the model. Set the following environment variable:

```
LIVESTOCK_MODEL_USE_CACHE=TRUE
CROP_MODEL_USE_CACHE=TRUE
```

If/when there is a need to refit the model with more recent data, the user can set the maximum training date as an environment variable, with the date in the format yyyy-mm. 

```
LIVESTOCK_MODEL_MAX_TRAINING_DATE="2022-09"
CROP_MODEL_MAX_TRAINING_DATE="2022-09"
```

To invalidate and refit the model:

1. `targets::tar_invalidate(repel_full_pipeline)` invalidates the full livestock pipeline including the model. `targets::tar_invalidate(repel_full_crop_pipeline)` invalidates the full crop pipeline including the model.

2. Set an environment variable to specify that the model should not be pulled from the cache. 

```
LIVESTOCK_MODEL_USE_CACHE=FALSE
CROP_MODEL_USE_CACHE=FALSE
```

3. `targets::tar_make(repel_full_pipeline)` refits the livestock model, which takes ~1 hr, and updates predictions and reports. `targets::tar_make(repel_full_crop_pipeline)` refits the crop model, which takes ~24 hrs, and updates predictions and reports.

4. Run the script `inst/cache_data_objects.R` to cache the new data objects. Within the script, you can specify a new GitHub version release to stash the data.

5. Set `LIVESTOCK_MODEL_USE_CACHE` or `CROP_MODEL_USE_CACHE` to `TRUE` to prevent the model from refitting when there are future data changes. With this setting, predictions can be updated with new data (see below), but not the model. 

# Model predictions

The targets `repel_predictions` and `repel_predictions_crop` provide model predictions for the full dataset, including 12 months ahead from when the data was last updated. 

The targets `repel_predictions_priority_diseases_usa` and `repel_predictions_priority_diseases_usa_crop` are the output of this function for priority diseases entering the US. 

The target `repel_predictions_crop_new_diseases` provides model predictions for priority crop diseases that were not reported in the data sources. 

The targets `repel_variable_importance_priority_diseases_usa` and `repel_variable_importance_priority_diseases_usa_crop` contain two data frames pertaining to priority diseases entering the US. `variable_importance` is the importance of each bilateral predictor variable for each month-country-disease prediction. `variable_importance_by_origin` disagreggates the variable importance by outbreak origin countries. 

## Updating model predictions

The model can generate predictions for 12 months ahead from when the data was last updated. To update the data, we suggest the following steps:

1. Make a backup of your existing `data-raw` directory.

2. Create an environment variable to allow Comtrade data to be redownloaded. This overrides the default time-saving behavior of skipping over files that have already been downloaded.
```
OVERWRITE_COMTRADE_LIVESTOCK_DOWNLOADED=TRUE
OVERWRITE_COMTRADE_CROP_DOWNLOADED=TRUE
```
3. Run `source("inst/invalidate_livestock_pipeline.R")` or `source("inst/invalidate_crop_pipeline.R")`. 

4. Set an environment variable for the current month, as follows. This tells the prediction function how far ahead it can make predictions.

```
LIVESTOCK_MODEL_LAST_DATA_UPDATE="2023-12"
CROP_MODEL_LAST_DATA_UPDATE="2023-12"
```

5. Run `targets::tar_make(repel_full_pipeline)` or `targets::tar_make(repel_full_crop_pipeline)` to rerun the full data download and processing pipeline. Note that this will not refit the model unless the user has `LIVESTOCK_MODEL_USE_CACHE`or `CROP_MODEL_USE_CACHE` environment variable set to FALSE (see above). 

# NowCast
The NowCast model estimates current and next-year livestock disease status (i.e., presence or absence) at the country level. It was previously developed under Phase I REPEL work, and was recreated under the current effort using updated WAHIS data. However, we found that including NowCast endemic disease status in the current REPEL travel model does not increase model performance, and therefore we did not include it in the final model. Here we document the code and reproducibility for this NowCast model.

NowCast steps are entirely in `tar_plans/_targets_nowcast.R`, which is currently commented out in the main `_targets.R` pipeline. The pipeline to run the NowCast consists of steps to process WAHIS six month disease status reports, augment with predictor variables that are also represented in the travel model (e.g., GDP, population), fit a boosted regression tree model, calculate validation statistics on holdout data, and generate predictions. 

To incorporate the NowCast model into the travel pipeline, the user must a) uncomment `tar_plans/_targets_nowcast.R`  in `_targets.R`, and b) specify `repel_nowcast_predictions` into the `nowcast` argument in the function `process_wahis_outbreaks()` within the file `tar_plans/_targets_data_processing_livestock.R`. 
